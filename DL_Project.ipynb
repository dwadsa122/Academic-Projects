{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "862ec1bd-6f07-43e9-a2ab-cf53c696f74d",
   "metadata": {},
   "source": [
    "# Fashion le Ardeur #\n",
    "\n",
    "**Group 1**\n",
    "\n",
    "    Colin Torralba\n",
    "\n",
    "    Daniel Kier Raluto\n",
    "    \n",
    "    Roxanne Bandivas\n",
    "    \n",
    "    Czarr Vic Pocol\n",
    "    \n",
    "    Jan Rey Viudor\n",
    "\n",
    "\n",
    "**Description:**\n",
    "\n",
    "    The very fabric of our society involes the very fabric of our clothes! Clothes, accessories, and etc. help define the culture/lifestyle of the people where unique characteristics/characters come alive that make our society feel alive! In this day and age, it is important to explore and widen our wardrobes, view different fashion senses from other cultures and let it evolve into something new! \n",
    "\n",
    "# Project Overview #\n",
    "    Fashion le Ardeur, is an image classification project that utilizes the Fashion Product Images(Small)(https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small) to create a CNN model that can determine what type of Fashion Product a certain image is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc17ede-6d17-4a85-8f37-68c0617b0e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision\n",
    "from torchvision import transforms \n",
    "from torchvision import models\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.backends.cudnn.deterministic=True\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada72e0a-4859-41a1-a225-d773f2d93642",
   "metadata": {},
   "source": [
    "# Dataset #\n",
    "    The Dataset was downloaded from kaggle. This is the small version and contains 44,000 images. It also contains a csv file that has columns of descriptions of all 44,000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb1f303-8f0f-482c-a84b-dac8ff6d7965",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The CSV file is currently in my CWD\n",
    "desc_file = pd.read_csv('styles.csv')\n",
    "desc_file['id'] = desc_file['id'].astype(str)\n",
    "\n",
    "\n",
    "# Opening the data\n",
    "# The image folder is currently in my laptop storage\n",
    "# Log 1, after a while of debugging and optimizing, I had to comeback here and fix things, some data were mismatching\n",
    "image = r'C:\\Users\\User\\Downloads\\Elec_4_ass\\images'\n",
    "img_file = os.listdir(image)\n",
    "img_df = pd.DataFrame({\"img_path\": img_file})\n",
    "img_df['id'] = img_df['img_path'].apply(lambda x: os.path.splitext(x)[0])\n",
    "merged_df = pd.merge(img_df, desc_file, on='id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a36fc2-7b33-487f-9d5c-6febd9188834",
   "metadata": {},
   "source": [
    "### Data Dictionary ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d91f18e-4d5d-4304-9846-a2ea353cf57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the data dictionary\n",
    "data_dict = {'img_path': merged_df['img_path'].apply(lambda x: os.path.join(image, x)).tolist(),\n",
    "             'type': merged_df['articleType'].tolist()}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a296a5e-80f0-493e-a597-8cd2c6d1c401",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93a09ef-c5a7-4e2b-a26f-1b87fd07c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['img_path'][19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b0d10d-203c-497f-b315-16711ea11288",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['type'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc19c81-db8e-4239-8cd3-fd5bbb316db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(data_dict['img_path'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73beaf4-02f7-4120-87fa-b27615ce1f2b",
   "metadata": {},
   "source": [
    "So we have constructed the data dictionary with efficiency. We have made a data dictionary containing the image path and its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc012c7-2192-47ed-8641-e347984708fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique types\n",
    "len(np.unique(data_dict['type']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc92dd8-1d02-43a1-808d-d500938b8c2c",
   "metadata": {},
   "source": [
    "### Class Map ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f880873-79df-48be-8b5a-2f65bdda9de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {label: x for x, label in enumerate(set(data_dict['type']))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97aaf7a-02ff-4b02-b98b-742798851647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1187da7e-a762-4060-890c-c15d41491f3a",
   "metadata": {},
   "source": [
    "### Custom Dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e8e123-2ebe-4d5a-b1be-ca2613295b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to create a custom dataset\n",
    "class Data(Dataset):\n",
    "    def __init__(self, data_dict, class_map = class_map, transformations = None):\n",
    "        self.data_dict = data_dict\n",
    "        self.class_map = class_map\n",
    "        self.transformations = transformations\n",
    "\n",
    "    def __len__(self):\n",
    "        return(len(self.data_dict['type']))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.data_dict['img_path'][idx])\n",
    "        label = self.class_map[self.data_dict['type'][idx]]\n",
    "        if self.transformations:\n",
    "            image = self.transformations(image)\n",
    "            label = torch.tensor(label)\n",
    "        return image, label\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d386e7-8069-4334-97d5-f8ce594a7d1c",
   "metadata": {},
   "source": [
    "### Some Utility Functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ffc7d8-05b8-4731-9660-da21aa889fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating seeds to make things fixed\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f241c764-bcd7-4817-be60-0a854437398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seed and creating my autobot\n",
    "set_seed(43)\n",
    "optimus = transforms.Compose([transforms.RandomRotation(5),transforms.RandomHorizontalFlip(0.5),transforms.Resize((80,60)),transforms.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513bd12d-4ed1-4148-bb5e-7a93c82de13c",
   "metadata": {},
   "source": [
    "### Initialization ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dcc3cc-6911-456e-882a-e8ec9cd96fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating my Data\n",
    "df = Data(data_dict, class_map, transformations = optimus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9e5424-b4cc-4eb0-8698-48255500e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "train_set,test_set,val_set = random_split(df, [0.8,0.1,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6f2a9e-fc73-4ab1-80e5-f88ce60b3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create batches! \n",
    "bs = 40\n",
    "\n",
    "#Data Loaders\n",
    "train_data = DataLoader(train_set, batch_size= bs, shuffle= True)\n",
    "test_data = DataLoader(test_set, batch_size= bs, shuffle= True)\n",
    "val_data = DataLoader(val_set, batch_size= bs, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e74a749-d4ff-440d-934b-6d150b18ca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = iter(train_data).__next__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ab0cb-3251-41cd-8a87-bfcc69e7f259",
   "metadata": {},
   "source": [
    "### Visualization ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1159d5a4-0bfc-430d-a253-b54d115c0143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to visualize\n",
    "vis = {d:f for f,d in df.class_map.items()}\n",
    "fig, axs = plt.subplots(nrows=2, ncols=5)\n",
    "\n",
    "c = 0\n",
    "for row in range(2):\n",
    "    for col in range(5):\n",
    "        axs[row, col].imshow(xb[c].permute(1,2,0))\n",
    "        axs[row, col].axis('off')\n",
    "        axs[row, col].set_title(vis[yb[c].item()])\n",
    "        c += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a4e0e-b563-4ff7-859d-0a60375b6f1c",
   "metadata": {},
   "source": [
    "# Time for Fashion le Passion! (CNN) #\n",
    "\n",
    "    We will begin to construct a small CNN model that can take on the task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b2dd4e-5c7a-4cd9-8ea2-e655c5364c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3544677-8e8d-423e-b9bc-cbee86dc7c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FLP(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(FLP,self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "#         self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "#         self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "#         self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1) \n",
    "#         self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1) \n",
    "#         self.fc1 = nn.Linear(512 * 2 * 1, 256) \n",
    "#         self.fc2 = nn.Linear(256, 128)  \n",
    "#         self.fc3 = nn.Linear(128, num_classes)\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = self.pool(F.relu(self.conv3(x)))\n",
    "#         x = self.pool(F.relu(self.conv4(x)))\n",
    "#         x = self.pool(F.relu(self.conv5(x)))\n",
    "#         #print(f\"Shape after pooling: {x.shape}\")\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         #print(f\"Shape after view: {x.shape}\")\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x)) \n",
    "#         x = self.fc3(x) \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87047065-dddc-47ea-ac5e-d5c6d2c85bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First model\n",
    "class FLP(nn.Module): \n",
    "    def __init__(self, num_classes): \n",
    "        super(FLP, self).__init__() \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1) \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0) \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1) \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1) \n",
    "        self.fc1 = nn.Linear(128 * 10 * 7, 64) \n",
    "        self.fc2 = nn.Linear(64, num_classes) \n",
    "    def forward(self, x): \n",
    "        x = self.pool(F.relu(self.conv1(x))) \n",
    "        x = self.pool(F.relu(self.conv2(x))) \n",
    "        x = self.pool(F.relu(self.conv3(x))) \n",
    "        # print(f\"Shape after pooling: {x.shape}\")  \n",
    "        x = x.view(x.size(0), -1) \n",
    "        # print(f\"Shape after view: {x.shape}\") \n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = self.fc2(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01938c32-b44c-4944-b332-42a995007fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed90103c-9100-4b23-8db5-227f04bb1d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ardeur means passion in french\n",
    "Ardeur = FLP(142)\n",
    "Ardeur = Ardeur.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7956b4-18b6-49ef-a625-3a832dbfe5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup loss function and optimizers and epoch!\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(Ardeur.parameters(), lr=0.0001)\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49732946-b736-4219-9350-d8d72a63d48e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training time! \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Starting epoch {epoch + 1}/{epochs}\")\n",
    "    Ardeur.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (x,y) in enumerate(train_data):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        #print(x.shape)\n",
    "        optimizer.zero_grad()\n",
    "        out = Ardeur(x)\n",
    "        #print(out.shape)\n",
    "        #print(y.shape)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_data)}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59103691-f453-44fd-ae8b-37316854bf39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Ardeur.eval() \n",
    "correct = 0 \n",
    "total = 0\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "for epoch in range(epochs):\n",
    "    with torch.no_grad():\n",
    "        for x,y in val_data:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out = Ardeur(x)\n",
    "            _, pred = torch.max(out.data,1)\n",
    "            #print(pred)\n",
    "            total += y.size(0)\n",
    "            correct += (pred == y).sum().item()\n",
    "            true_labels.extend(y.cpu().numpy())\n",
    "            pred_labels.extend(pred.cpu().numpy())\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "true_labels = torch.tensor(true_labels)\n",
    "pred_labels = torch.tensor(pred_labels)\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(true_labels,pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131a90f5-1a4f-44ac-9dac-757ef09496aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tes = Image.open(data_dict['img_path'][9999])\n",
    "def predict(model, img):\n",
    "    img_tens = transforms.ToTensor()(img)\n",
    "    img_tens = img_tens.to(device)\n",
    "    plt.imshow(img)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tens.unsqueeze(0))\n",
    "        _, pred = torch.max(output, 1)\n",
    "        predicted_class_name = [k for k, v in class_map.items() if v == pred.item()][0]\n",
    "        print(\"Predicted class:\", predicted_class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdc2f6f-dead-402f-b2be-8e857cfc2416",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(Ardeur,tes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa112ac9-3c17-4a21-a309-7d2e0ee29b57",
   "metadata": {},
   "source": [
    "# Assumptions and Comments #\n",
    "    The model has an average accuracy of 82% which makes it a highly functioning model. It is a simple model for a somewhat large dataset. Honestly quite impressed. One of my assumptions is that this model can now accurately predict what fashion product is that based on an image. \n",
    "\n",
    "# Recommendations # \n",
    "    Simply Hyperparameter Tuning and the data itself. This data is only the small version, the actual dataset contains about 140k images that can be useful and might yield a more accurate model. For hyperparameter tuning, it is recommended to tweak certain numbers to achieve varying results.\n",
    "\n",
    "# Conclusion #\n",
    "    -> Downloaded the data\n",
    "    -> Created a data dictionary containing the image paths and their types by matching the img id with the id from the csv file by turning the image folder into a dataframe\n",
    "    -> Created the classmap based on the dictionary\n",
    "    -> Created the Custom Dataset\n",
    "    -> Reused some utility functions\n",
    "    -> Initialized the data that will be used for training the model\n",
    "    -> Visualized some samples # Note that there was an error in the visualization, hence we had to debug the data dictionary as data mismatch were observed\n",
    "    -> Fashion le Ardeur!\n",
    "    -> Training\n",
    "    -> Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66601b45-8203-479e-b7f1-c53e00363a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d7a09-4656-433b-9628-932dcb41dd83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdafb5f-1ed2-422c-9ce6-44c28b88c459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4fa71c-b005-4199-b6eb-9018bf4c81ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
